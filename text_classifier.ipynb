{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecf9cb3",
   "metadata": {},
   "source": [
    "# 0. import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f82273d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch             # pytorch framework for CNN\n",
    "import numpy as np       # numpy for fast multidimensional array manipulation\n",
    "import random            \n",
    "import pandas as pd      # pandas to deal with datasets\n",
    "import nltk              # natural language toolkit to preprocess the text\n",
    "import re                # regular expression to preprocess text\n",
    "import collections\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea5df89",
   "metadata": {},
   "source": [
    "## 0.1 set device and default tensor type\n",
    "**set computing device to GPU if exists (change runtime type from colab)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c78be011",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "device = torch.device(\"cuda\" if cuda and torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c346d0",
   "metadata": {},
   "source": [
    "**set default tensor type to cuda float tensor if availabe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "99fe080d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "if device == \"cuda\":\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec0d76e",
   "metadata": {},
   "source": [
    "# 1. load data\n",
    "## 1.1 create dataframe for the dataset using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2a21371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/SamirGouda/text_classifier/398183a3f714225e827afe0350550e751e6df33b/news.csv\"\n",
    "df = pd.read_csv(url, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279df48",
   "metadata": {},
   "source": [
    "## 1.2 shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "55d2c916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LEONARD FRUSTRATED FOR WILKINSON</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BEST OF ECT NEWS Biometrics: ThinkPad and Beyond</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lighter-Hit Fla. Area Gets  #36;21.5 From FEMA...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>House Toughens Penalties on P2Ps</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Humans may need fewer genes than thought</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  category\n",
       "0                   LEONARD FRUSTRATED FOR WILKINSON    Sports\n",
       "1   BEST OF ECT NEWS Biometrics: ThinkPad and Beyond  Sci/Tech\n",
       "2  Lighter-Hit Fla. Area Gets  #36;21.5 From FEMA...     World\n",
       "3                   House Toughens Penalties on P2Ps  Sci/Tech\n",
       "4           Humans may need fewer genes than thought  Sci/Tech"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "# examine the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4736d31",
   "metadata": {},
   "source": [
    "# 2. preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37680736",
   "metadata": {},
   "source": [
    "## 2.1 import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "dbd4a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # not used yet\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab1c25",
   "metadata": {},
   "source": [
    "### 2.1.1 download stopwords and create stopwords and stemmer instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cd4a3ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SamirGouda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "porter = PorterStemmer() # not used in current version\n",
    "# check the first 5 stopwords\n",
    "print(STOPWORDS[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27127932",
   "metadata": {},
   "source": [
    "## 2.2 preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b452c6a",
   "metadata": {},
   "source": [
    "### 2.2.1 preprocess function\n",
    "**removes stopwords, words in paranthesis, spaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bc436141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stopwords=STOPWORDS):\n",
    "    \"\"\" conditional preprocessing on the given text \"\"\"\n",
    "    # transform upper case letters to lower\n",
    "    text = text.lower()\n",
    "    # remove stopwords using regular expressions\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords)+ r')\\b\\s*')   # create pattern instance of stopwords\n",
    "    text = pattern.sub('', text)                                     # replace stopwords with empty char\n",
    "    # remove words in paranthesis\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)                  # removes punctuation\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)                        # remove non alphanumeric chars\n",
    "    text = re.sub(' +', ' ', text)                                   # remove multiple spaces\n",
    "    text =text.strip()                                               # remove leading and trailing spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc4d47",
   "metadata": {},
   "source": [
    "### 2.2.2 apply preprocess func on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3fb2f35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEONARD FRUSTRATED FOR WILKINSON\n",
      "\n",
      "leonard frustrated wilkinson\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.title = preprocessed_df.title.apply(preprocess)\n",
    "print(f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f3c753",
   "metadata": {},
   "source": [
    "## 2.3 split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b840e8d",
   "metadata": {},
   "source": [
    "### 2.3.1 splitting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bb1390a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def train_val_test_split(X, y, train_size):\n",
    "    X_train, X_, y_train, y_= train_test_split(X, y, train_size= train_size, stratify= y) \n",
    "    X_val, X_test, y_val, y_test= train_test_split(X_, y_, train_size= 0.5, stratify= y_)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebf7d2",
   "metadata": {},
   "source": [
    "### 2.3.2 split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e8537944",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessed_df['title'].values\n",
    "y = preprocessed_df['category'].values\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X, y, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c7e9d9",
   "metadata": {},
   "source": [
    "## 2.4 encode data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d088e1",
   "metadata": {},
   "source": [
    "### 2.4.1 label encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7acd362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(object):\n",
    "    def __init__(self, class_to_index = {}):\n",
    "        self.class_to_index = class_to_index\n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.class_to_index)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Label Encoder (Num_Classes = {})>\".format(len(self))\n",
    "    \n",
    "    def fit(self, y):\n",
    "        classes = np.unique(y)\n",
    "        for i, class_ in enumerate(classes):\n",
    "            self.class_to_index[class_] = i \n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "        return self\n",
    "    \n",
    "    def encode(self, y):\n",
    "        encoded = np.zeros(len(y), dtype=int)\n",
    "        for i, class_ in enumerate(y):\n",
    "            encoded[i] = self.class_to_index[class_]\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, y):\n",
    "        classes = []\n",
    "        for idx in y:\n",
    "            classes.append(self.index_to_class[idx])\n",
    "        return classes\n",
    "    \n",
    "    def save(self, fp):\n",
    "        with open(fp, 'w') as fp:\n",
    "            contents = {'class_to_index': self.class_to_index}\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "            \n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, 'r') as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798b30b",
   "metadata": {},
   "source": [
    "### 2.4.2 encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "eee744a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Business', 'Sci/Tech', 'Sports', 'World']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "classes = label_encoder.classes\n",
    "NUM_CLASSES = len(label_encoder)\n",
    "print(classes)\n",
    "# convert labels to tokens\n",
    "y_train = label_encoder.encode(y_train)\n",
    "y_val = label_encoder.encode(y_val)\n",
    "y_test = label_encoder.encode(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "78893721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3.7037037037037037e-05, 1: 3.7037037037037037e-05, 2: 3.7037037037037037e-05, 3: 3.7037037037037037e-05}\n"
     ]
    }
   ],
   "source": [
    "# class weights\n",
    "counts = np.bincount(y_train)\n",
    "class_weights = {i: 1.0/ count for i, count in enumerate(counts)}\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c2345",
   "metadata": {},
   "source": [
    "## 2.5 Tokenize the features\n",
    "### 2.5.1 tokenize class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e9167435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import take # not used in this version\n",
    "from collections import Counter\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, char_level, num_tokens=None, pad_token='<PAD>', oov_token='<UNK>', token_to_index= None):\n",
    "        self.char_level = char_level\n",
    "        self.seperator = '' if self.char_level else ' '\n",
    "        if num_tokens: num_tokens -=2\n",
    "        self.num_tokens = num_tokens\n",
    "        self.pad_token = pad_token\n",
    "        self.oov_token = oov_token\n",
    "        if not token_to_index:\n",
    "            token_to_index = {pad_token:0, oov_token:1}\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = {v:k for k, v in token_to_index.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Tokenizer (num_tokens= {})>\".format(len(self))\n",
    "  \n",
    "    def fit_on_texts(self, texts):\n",
    "        if not self.char_level:\n",
    "            texts = [text.split(\" \") for text in texts]\n",
    "        all_tokens = [token for text in texts for token in text]\n",
    "        counts = Counter(all_tokens).most_common(self.num_tokens)\n",
    "        self.min_token_freq = counts[-1][1]\n",
    "        for token, count in counts:\n",
    "            index = len(self)\n",
    "            self.token_to_index[token] = index\n",
    "            self.index_to_token[index] = token\n",
    "        return self\n",
    "\n",
    "    def texts_to_sequences(self,  texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            if not self.char_level:\n",
    "                text = text.split(\" \")\n",
    "            sequence = []\n",
    "            for token in text:\n",
    "                sequence.append(self.token_to_index.get(token, self.token_to_index[self.oov_token]))\n",
    "            sequences.append(np.asarray(sequence))\n",
    "        return sequences\n",
    "  \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = []\n",
    "            for index in sequence:\n",
    "                text.append(self.index_to_token.get(index, self.oov_token))\n",
    "            texts.append(self.seperator.join([token for token in text]))\n",
    "        return texts\n",
    "\n",
    "    def save(self, fp):\n",
    "        with open(fp, 'w') as fp:\n",
    "            contents = {'char_level': self.char_level, 'oov_token': self.oov_token, 'token_to_index': self.token_to_index}\n",
    "            json.dump(contents, fp, indent=2, sort_keys=False)\n",
    "  \n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, 'r') as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b74eb",
   "metadata": {},
   "source": [
    "### 2.5.2 tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "edb530e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<PAD>', 0), ('<UNK>', 1), ('39', 2), ('b', 3), ('gt', 4)]\n",
      "least frequency tokens: 215\n",
      "text to indices\n",
      "preprocess astros 7 <UNK> 5\n",
      "tokenized [420  98   1  76]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(char_level=False, num_tokens=500)\n",
    "tokenizer.fit_on_texts(texts=X_train)\n",
    "# sample of token\n",
    "print(take(5, tokenizer.token_to_index.items()))\n",
    "print(f\"least frequency tokens: {tokenizer.min_token_freq}\") # to adjust num_tokens\n",
    "# convert text to sequences of indices\n",
    "X_train = tokenizer.texts_to_sequences(texts= X_train)\n",
    "X_val = tokenizer.texts_to_sequences(texts= X_val)\n",
    "X_test = tokenizer.texts_to_sequences(texts= X_test)\n",
    "preprossed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\n",
    "print(\"text to indices\\n\"\n",
    "      f\"preprocess {preprossed_text}\\n\"\n",
    "      f\"tokenized {X_train[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e4136",
   "metadata": {},
   "source": [
    "## 2.6 onehot encoding for tokens\n",
    "### 2.6.1 onehot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "44112a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequence, num_classes):\n",
    "    one_hot = np.zeros((len(sequence), num_classes))\n",
    "    for i, value in enumerate(sequence):\n",
    "        one_hot[i, value] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f0c4b",
   "metadata": {},
   "source": [
    "### 2.6.2 convert tokens to one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "64c2de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "X_train = [to_categorical(seq, num_classes=vocab_size) for seq in X_train]\n",
    "X_val = [to_categorical(seq, num_classes=vocab_size) for seq in X_val]\n",
    "X_test = [to_categorical(seq, num_classes=vocab_size) for seq in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3db7dc",
   "metadata": {},
   "source": [
    "## 2.7 pad the features\n",
    "**to have same length**\n",
    "### 2.7.1 pad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "10201fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_seq_len=0):\n",
    "    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n",
    "    num_classes = sequences[0].shape[-1]\n",
    "    padded_sequences = np.zeros((len(sequences), max_seq_len, num_classes))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        padded_sequences[i][:len(sequence)] = sequence\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f231dcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 500) (6, 500) (5, 500)\n",
      "(108000, 19, 500)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape, X_train[1].shape, X_train[2].shape)\n",
    "print(pad_sequences(X_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a9d0e1",
   "metadata": {},
   "source": [
    "## 2.7 create dataset from dataframe\n",
    "### 2.7.1 dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9795ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, max_filter_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.max_filter_size = max_filter_size\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "  \n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)}>\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return [X, y]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\" processing on batch \n",
    "        X, y = [], []\n",
    "        for _data, _label in batch:\n",
    "          X.append(_data)\n",
    "          y.append(_label)\n",
    "        X = pad_sequences(X, max_seq_len=self.max_filter_size)\n",
    "        return torch.FloatTensor(X), torch.LongTensor(y)\n",
    "\n",
    "        \"\"\"\n",
    "        batch = np.array(batch, dtype=object)\n",
    "        X = batch[:,0]\n",
    "        y = np.stack(batch[:,1], axis=0)\n",
    "        # pad sequences\n",
    "        X = pad_sequences(X, max_seq_len=self.max_filter_size)\n",
    "\n",
    "        # cast to tensors\n",
    "        X = torch.FloatTensor(X.astype(np.int32))\n",
    "        y = torch.LongTensor(y.astype(np.int32))\n",
    "        return X, y\n",
    "    \n",
    "\n",
    "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
    "        return torch.utils.data.DataLoader(dataset= self, batch_size=batch_size, shuffle=shuffle,\n",
    "                                       drop_last=drop_last, pin_memory=True, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0ddb0",
   "metadata": {},
   "source": [
    "### 2.7.2 create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c03042c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SIZE = 1 # unigram\n",
    "train_dataset = Dataset(X_train, y_train, max_filter_size=FILTER_SIZE)\n",
    "val_dataset = Dataset(X_val, y_val, max_filter_size=FILTER_SIZE)\n",
    "test_dataset = Dataset(X_test, y_test, max_filter_size=FILTER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b9c2a",
   "metadata": {},
   "source": [
    "### 2.7.3 create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c74aba9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([64, 9, 500])\n",
      "y: 0\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataloader = train_dataset.create_dataloader(BATCH_SIZE)\n",
    "val_dataloader = val_dataset.create_dataloader(BATCH_SIZE)\n",
    "test_dataloader = test_dataset.create_dataloader(BATCH_SIZE)\n",
    "batch_X, batch_y = next(iter(test_dataloader))\n",
    "print(f\"X: {batch_X.size()}\\n\"  f\"y: {batch_y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ac989",
   "metadata": {},
   "source": [
    "# 3. build cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4c212bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e0a29",
   "metadata": {},
   "source": [
    "## 3.1 CNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "27f067bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, filter_size, num_filters, hidden_dim, dropout_prop):\n",
    "        super(CNN, self).__init__()\n",
    "        # conv layers\n",
    "        self.conv = nn.Conv1d(in_channels=input_size, out_channels=num_filters, kernel_size=filter_size, padding=\"same\")\n",
    "        self.bn = nn.BatchNorm1d(num_features=num_filters)\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=num_filters, out_features=hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_prop)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_normal_(self.conv.weight, gain=1)\n",
    "        nn.init.xavier_normal_(self.fc1.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_normal_(self.fc2.weight, gain=1)\n",
    "        \n",
    "    def forward(self, inputs, channel_first=False, apply_softmax=False):\n",
    "        X = inputs\n",
    "        if not channel_first:\n",
    "            X = torch.transpose(X, 2, 1)     # rearrange to have channel first (m, c, w, h)\n",
    "        X = self.conv(X)\n",
    "        X = F.max_pool1d(X, X.size(2)).squeeze(2)\n",
    "        # fully connected layers\n",
    "        X = self.fc1(X)\n",
    "        X = self.dropout(X)\n",
    "        X = self.fc2(X)\n",
    "        if apply_softmax:\n",
    "            X = F.softmax(X, dim=1)\n",
    "        return X\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ecb176",
   "metadata": {},
   "source": [
    "### 3.1.1 initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "118cc4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_parameters of CNN(\n",
      "  (conv): Conv1d(500, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=32, out_features=100, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "NUM_FILTERS = 50\n",
    "HIDDEN_DIM = 100\n",
    "DROPOUT_P = 0.1\n",
    "filter_size = 3\n",
    "num_filters = 32\n",
    "\n",
    "model = CNN(vocab_size, NUM_CLASSES, filter_size, num_filters, HIDDEN_DIM, DROPOUT_P)\n",
    "model = model.to(device)\n",
    "print(model.named_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a6ac89",
   "metadata": {},
   "source": [
    "## 3.2 Trainer\n",
    "### 3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5e3ff025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, device):\n",
    "        # set params\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def init_loss_fn(self, class_weight):\n",
    "        weight = torch.Tensor(list(class_weight.values())).to(self.device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    " \n",
    "    def init_optimizer(self, optimizer_type, learning_rate, lambda_):\n",
    "        self.optimizer = optimizer_type(params=self.model.parameters(), lr=learning_rate, weight_decay=lambda_)\n",
    "\n",
    "    def init_scheduler(self, mode, factor, patience):\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=mode, factor=factor, patience=patience)\n",
    "\n",
    "\n",
    "    def train_step(self, dataloader):\n",
    "        # set model to train mode\n",
    "        self.model.train()  \n",
    "        loss = 0.0\n",
    "        # iterate over train minibatches in dataloader\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # step\n",
    "            batch = [item.to(self.device) for item in batch] # set device\n",
    "            features, targets = batch[0], batch[-1]                     # features = inputs, labels = targets = outputs\n",
    "            self.optimizer.zero_grad()                                    # reset gradients\n",
    "            A = self.model.forward(features)                              # forwardprop\n",
    "            J = self.loss_fn(A, targets)                                  # cost function\n",
    "            J.backward()                                                  # backprop\n",
    "            self.optimizer.step()                                         # update params\n",
    "\n",
    "            # cumulative metrics\n",
    "            loss += (J.detach().item()- loss)/ (i+1)\n",
    "\n",
    "        return loss  \n",
    "\n",
    "    def eval_step(self, dataloader):\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        loss = 0.0\n",
    "        y_trues, y_probs = [], []\n",
    "        # iterate over val minibatches in dataloader\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # step\n",
    "            batch = [item.to(self.device) for item in batch] # set device\n",
    "            features, labels = batch[0], batch[-1]                        # features = inputs, labels = targets = outputs\n",
    "            A = self.model.forward(features)                              # forwardprop\n",
    "            J = self.loss_fn(A, labels).item()                            # cost function\n",
    "            # cumulative metrics\n",
    "            loss += (J- loss)/ (i+1)\n",
    "            # store outputs\n",
    "            y_prob = torch.sigmoid(A).cpu().detach().numpy()\n",
    "            y_probs.extend(y_prob)\n",
    "            y_trues.extend(labels.cpu().detach().numpy())\n",
    "        return loss, np.vstack(y_trues), np.vstack(y_probs)\n",
    "\n",
    "    def predict_step(self, dataloader):\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        y_probs = []\n",
    "        # iterate over val minibatches\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                features, labels = batch[0], batch[-1]                             # features = inputs, labels = targets = outputs\n",
    "                A = self.model.forward(features, apply_softmax=True)               # forwardprop\n",
    "                # store outputs\n",
    "                y_probs.extend(A)\n",
    "        return np.vstack(y_probs)\n",
    "\n",
    "    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n",
    "        best_val_loss = np.inf\n",
    "        for epoch in range(num_epochs):\n",
    "            # steps\n",
    "            train_loss = self.train_step(dataloader= train_dataloader)\n",
    "            val_loss, _, _ = self.eval_step(dataloader= val_dataloader)\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "          # early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = self.model\n",
    "                patience_ = patience   # reset patience\n",
    "            else:\n",
    "                patience_ -= 1 \n",
    "            if not patience_:\n",
    "                print(\"early stopping!\")\n",
    "                break\n",
    "\n",
    "          # logging\n",
    "            print(\n",
    "              f\"epoch: {epoch},  \"\n",
    "              f\"train_loss: {train_loss:.5f},  \"\n",
    "              f\"val_loss: {val_loss:.5f},  \"\n",
    "              f\"lr: {self.optimizer.param_groups[0]['lr']:.2E},  \"\n",
    "              f\"patience: {patience_},  \"\n",
    "            )\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cb55f",
   "metadata": {},
   "source": [
    "### 3.2.2 instanitate trainer instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f711ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 5\n",
    "NUM_EPOCHS = 10\n",
    "# initialize trainer\n",
    "trainer = Trainer(model, device)\n",
    "trainer.init_loss_fn(class_weight=class_weights)\n",
    "trainer.init_optimizer(optim.Adam, LEARNING_RATE, lambda_=0)\n",
    "trainer.init_scheduler(mode='min', factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae87fd4",
   "metadata": {},
   "source": [
    "# 4. train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "eeaf609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,  train_loss: 0.83996,  val_loss: 0.78428,  lr: 1.00E-03,  patience: 5,  \n",
      "epoch: 1,  train_loss: 0.74631,  val_loss: 0.78112,  lr: 1.00E-03,  patience: 5,  \n",
      "epoch: 2,  train_loss: 0.72226,  val_loss: 0.78078,  lr: 1.00E-03,  patience: 5,  \n",
      "epoch: 3,  train_loss: 0.70441,  val_loss: 0.78397,  lr: 1.00E-03,  patience: 4,  \n",
      "epoch: 4,  train_loss: 0.68958,  val_loss: 0.78853,  lr: 1.00E-03,  patience: 3,  \n",
      "epoch: 5,  train_loss: 0.67701,  val_loss: 0.79378,  lr: 1.00E-03,  patience: 2,  \n",
      "epoch: 6,  train_loss: 0.66626,  val_loss: 0.79943,  lr: 1.00E-04,  patience: 1,  \n",
      "early stopping!\n"
     ]
    }
   ],
   "source": [
    "best_model = trainer.train(NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9432e5",
   "metadata": {},
   "source": [
    "# 5. evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c39bbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def get_metrics(y_true, y_pred, classes):\n",
    "    performance ={'overall':{}, 'class':{}}\n",
    "    metrics= precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    performance['overall']['precision'] = metrics[0]\n",
    "    performance['overall']['recall'] = metrics[1]\n",
    "    performance['overall']['f1'] = metrics[2]\n",
    "    performance['overall']['num_samples'] = np.float64(len(y_true))\n",
    "\n",
    "    metrics= precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    for i in range(len(classes)):\n",
    "        performance['class'][classes[i]] = {'precision':metrics[0][i], 'recall':metrics[1][i], 'f1':metrics[2][i], 'num_samples': np.float64(metrics[3][i])}\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a7b140fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"overall\": {\n",
      "        \"precision\": 0.7049331710012828,\n",
      "        \"recall\": 0.6896666666666667,\n",
      "        \"f1\": 0.68882045165279,\n",
      "        \"num_samples\": 6000.0\n",
      "    },\n",
      "    \"class\": {\n",
      "        \"Business\": {\n",
      "            \"precision\": 0.7178487918939984,\n",
      "            \"recall\": 0.614,\n",
      "            \"f1\": 0.6618756737333813,\n",
      "            \"num_samples\": 1500.0\n",
      "        },\n",
      "        \"Sci/Tech\": {\n",
      "            \"precision\": 0.7303727200634417,\n",
      "            \"recall\": 0.614,\n",
      "            \"f1\": 0.6671495834842448,\n",
      "            \"num_samples\": 1500.0\n",
      "        },\n",
      "        \"Sports\": {\n",
      "            \"precision\": 0.5946579194001874,\n",
      "            \"recall\": 0.846,\n",
      "            \"f1\": 0.6984039625756743,\n",
      "            \"num_samples\": 1500.0\n",
      "        },\n",
      "        \"World\": {\n",
      "            \"precision\": 0.7768532526475038,\n",
      "            \"recall\": 0.6846666666666666,\n",
      "            \"f1\": 0.7278525868178596,\n",
      "            \"num_samples\": 1500.0\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "trainer_ = Trainer(best_model, device)\n",
    "trainer_.init_loss_fn(class_weights)\n",
    "trainer_.init_optimizer(optim.Adam, learning_rate=LEARNING_RATE, lambda_=0)\n",
    "trainer_.init_scheduler(mode= 'min', factor=0.1, patience=PATIENCE)\n",
    "\n",
    "test_loss, y_true, y_prob = trainer_.eval_step(test_dataloader)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "performance = get_metrics(y_test, y_pred, classes= classes)\n",
    "print(json.dumps(performance, indent= 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb28119",
   "metadata": {},
   "source": [
    "# 6. test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c51134b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propability_distribution(y_prob, classes):\n",
    "    results ={}\n",
    "    for i, class_ in enumerate(classes):\n",
    "        results[class_] = np.float64(y_prob[0][i])\n",
    "    sorted_results = {k:v for k, v in sorted(results.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "18ea1dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['day new <UNK> stock market go <UNK>']\n"
     ]
    }
   ],
   "source": [
    "# inference on test sample\n",
    "test_text = \"What a day for the new york stock market to go bust!\"\n",
    "sequences = tokenizer.texts_to_sequences([preprocess(test_text)])\n",
    "print(tokenizer.sequences_to_texts(sequences))\n",
    "X_infer = [to_categorical(sequence, len(tokenizer)) for sequence in sequences]\n",
    "y_infer = label_encoder.encode([label_encoder.classes[0]]* len(X_infer))\n",
    "infer_dataset = Dataset(X_infer, y_infer, max_filter_size= FILTER_SIZE)\n",
    "infer_dataloader = infer_dataset.create_dataloader(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ab75ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_infer_prob = trainer_.predict_step(infer_dataloader)\n",
    "y_infer_pred = np.argmax(y_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c0151b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Business\": 0.9245333075523376,\n",
      "  \"Sci/Tech\": 0.06664972007274628,\n",
      "  \"World\": 0.008696649223566055,\n",
      "  \"Sports\": 0.00012020469148410484\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# class distribution\n",
    "prob_dist = get_propability_distribution(y_infer_prob, classes)\n",
    "print(json.dumps(prob_dist, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc8de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
